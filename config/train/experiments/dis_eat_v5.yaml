# @package _global_
defaults:
    - override /datamodule: basic
    - override /frontend: dis_eat
    - override /trainer: default


label_dict_path:
  main_label: "labels/${dcase}/machine_section_attr_domain.json"

model_ver: "all"

trainer:
  max_epochs: 10
  # accumulate_grad_batches: 8

datamodule:
  batch_size: 8

frontend:
  model_cfg:
    extractor_cfg:
      ckpt_path: "pretrained_models/eat/EAT-base_epoch10_pt.pt"
      split10sec: false
      model_cfg:
        sec: ${datamodule.sec}
    loss_cfg:
      tgt_class: asdit.losses.AdaCos
      dynamic: false
      trainable: true
    augmentation_cfg_list: []
  optim_cfg:
    tgt_class: torch.optim.AdamW
    lr: 1.0e-4
  scheduler_cfg:
    tgt_class: timm.scheduler.CosineLRScheduler
    t_initial: 20000
    lr_min: 1e-7
    warmup_t: 2000
    warmup_lr_init: 0
  partially_saved_param_list: ["lora"]
