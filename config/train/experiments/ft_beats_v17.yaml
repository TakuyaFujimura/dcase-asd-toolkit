# @package _global_
defaults:
    - override /datamodule: basic
    - override /frontend: ft_beats
    - override /trainer: default


label_dict_path:
  main_label: "labels/${dcase}/machine_section_attr_domain.json"

model_ver: "all"

trainer:
  max_epochs: 25
  # accumulate_grad_batches: 8

datamodule:
  batch_size: 8

frontend:
  model_cfg:
    loss_cfg:
      tgt_class: asdit.losses.AdaCos
      dynamic: false
      trainable: true
    extractor_cfg:
      sr: ${datamodule.sr}
      embed_size: 256
      ckpt_path: "pretrained_models/beats/BEATs_iter3.pt"
      lora_cfg:
        r: 64
        target_modules: ["q_proj", "v_proj", "out_proj"]
      last_layer: "attn_stat_pool"
      update_cfg:
        dropout_input: 0.3
        dropout: 0.0
        attention_dropout: 0.0
    augmentation_cfg_list: []
  optim_cfg:
    tgt_class: torch.optim.AdamW
    lr: 5.0e-5
  scheduler_cfg:
    tgt_class: timm.scheduler.CosineLRScheduler
    t_initial: 50000
    lr_min: 1e-5
    warmup_t: 5000
    warmup_lr_init: 0
  partially_saved_param_list: ["lora"]
